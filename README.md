# Deep Learning specialization by Deeplearning.ai

## [Neural Networks & Deep Learning](https://www.coursera.org/account/accomplishments/verify/J8L6HGRSHT9C)
The coursework provides a foundational understanding of a neural network. Different NN-based classifier models were built from scratch, using first principles - forward & backward propagation, calculating gradients and optimizing using gradient descent approach, **using mostly NumPy**. Classifiers built:
1. Logistic regression classifier (as a single layer neural network) - test accuracy of ~70% [notebook](https://github.com/uditgt/DeepLearning/blob/main/Logistic%20Regression%20from%20scratch.ipynb)
2. Shallow neural network - test accuracy of ~72% [notebook](https://github.com/uditgt/DeepLearning/blob/main/Neural%20Network%20from%20scratch.ipynb)
3. Deep neural network - test accuracy of 82% [notebook](https://github.com/uditgt/DeepLearning/blob/main/Deep%20Neural%20Network%20from%20scratch.ipynb)

Focus of these exercises was to carefully piecing together the different equations for forward and backward pass, in vectorized form, to build a functioning neural network.


## References:
* Dropout regularization, Gofrrey Hinton (2014) [link](https://jmlr.org/papers/v15/srivastava14a.html)
* Xavier initialization (2010) [link](https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)
